{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "725896d79ad745b5",
   "metadata": {},
   "source": [
    "# Exercises 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "592a5ab221a7227",
   "metadata": {},
   "source": [
    "### Explain in your own words: What is the the difference between the html page and the wiki-source"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fda0e6198adcad84",
   "metadata": {},
   "source": [
    "The HTML is the standard markup language for documents designed to be displayed in a web browser. A HTML page is a document which we can find on the web browser and is written in HTML.\n",
    "Wikisource is an online wiki-basedÂ digital library.\n",
    "The HTML page is only a document which can be found on the web browser. The Wikisource stores and organizes numorous HTML page in a graph structure, so that it can be find easily."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ebd8a329bcf73c",
   "metadata": {},
   "source": [
    "### What are the various parameters you can set for a query of the wikipedia api?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d250d68c9d879768",
   "metadata": {},
   "source": [
    "The parameters we can set for a query of the wikipedia api can be found here: https://www.mediawiki.org/wiki/API:Properties#revisions_.2F_rv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e34cdb1ef19d5b0e",
   "metadata": {},
   "source": [
    "### Write your own little notebook to download wikipedia pages based on the video above. Download the source for your 4 favorite wikipedia pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3303d783-fa02-4d7c-aa29-8a5b91d80158",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Processing: Artificial Intelligence ---\n",
      "Downloading: Artificial Intelligence\n",
      "Saved: wikipedia_pages/Artificial Intelligence.html\n",
      "\n",
      "--- Processing: Black Holes ---\n",
      "Downloading: Black Holes\n",
      "Saved: wikipedia_pages/Black Holes.html\n",
      "\n",
      "--- Processing: Leonardo da Vinci ---\n",
      "Downloading: Leonardo da Vinci\n",
      "Saved: wikipedia_pages/Leonardo da Vinci.html\n",
      "\n",
      "--- Processing: Ancient Egypt ---\n",
      "Downloading: Ancient Egypt\n",
      "Saved: wikipedia_pages/Ancient Egypt.html\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import os\n",
    "from urllib.parse import quote\n",
    "import time\n",
    "import json\n",
    "\n",
    "class WikipediaDownloader:\n",
    "    def __init__(self):\n",
    "        self.base_url = \"https://en.wikipedia.org/api/rest_v1/page/html/\"\n",
    "        self.session = requests.Session()\n",
    "        self.session.headers.update({\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "        })\n",
    "    \n",
    "    def download_page_html(self, title):\n",
    "        try:\n",
    "            # Clean the title for URL\n",
    "            clean_title = quote(title.replace(' ', '_'))\n",
    "            url = f\"{self.base_url}{clean_title}\"\n",
    "            \n",
    "            print(f\"Downloading: {title}\")\n",
    "            response = self.session.get(url)\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            return response.text\n",
    "        except requests.RequestException as e:\n",
    "            print(f\"Error downloading {title}: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def save_page(self, title, content):\n",
    "        if content is None:\n",
    "            return False\n",
    "        \n",
    "        # Create directory if it doesn't exist\n",
    "        os.makedirs(\"wikipedia_pages\", exist_ok=True)\n",
    "        \n",
    "        # Clean filename\n",
    "        safe_filename = \"\".join(c for c in title if c.isalnum() or c in (' ', '-', '_')).rstrip()\n",
    "        filename = f\"wikipedia_pages/{safe_filename}.html\"\n",
    "        \n",
    "        try:\n",
    "            with open(filename, 'w', encoding='utf-8') as f:\n",
    "                f.write(content)\n",
    "            print(f\"Saved: {filename}\")\n",
    "            return True\n",
    "        except IOError as e:\n",
    "            print(f\"Error saving {title}: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def download_and_save(self, title):\n",
    "        content = self.download_page_html(title)\n",
    "        \n",
    "        if content:\n",
    "            return self.save_page(title, content)\n",
    "        return False\n",
    "\n",
    "downloader = WikipediaDownloader()\n",
    "\n",
    "favorite_pages = [\n",
    "    \"Artificial Intelligence\",\n",
    "    \"Black Holes\",\n",
    "    \"Leonardo da Vinci\", \n",
    "    \"Ancient Egypt\"\n",
    "]\n",
    "\n",
    "for page_title in favorite_pages:\n",
    "    print(f\"\\n--- Processing: {page_title} ---\")\n",
    "    \n",
    "    # Download HTML version\n",
    "    success_html = downloader.download_and_save(page_title)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7eb99106ddb2f61",
   "metadata": {},
   "source": [
    "---\n",
    "# Exercises 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c304879a59e5c0",
   "metadata": {},
   "source": [
    "### List three different real networks and state the nodes and links for each of them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8cb931460eccb4e",
   "metadata": {},
   "source": [
    "* Facebook connections\n",
    "    * *Nodes*: People\n",
    "    * *Links*: Friendlist\n",
    "* Internet\n",
    "    * *Nodes*: Routers and Switches\n",
    "    * *Links*: Network cable\n",
    "* Public transportation system\n",
    "    * *Nodes*: Stations\n",
    "    * *Links*: Roads"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ed0ddbfbb9621c",
   "metadata": {},
   "source": [
    "### Tell us of the network you are personally most interested in (a fourth one). Address the following questions:\n",
    "* What are its nodes and links?\n",
    "* How large is it?\n",
    "* Can be mapped out?\n",
    "* Does it evolve over time?\n",
    "* Are there processes occurring ON the network? (information spreading, for example)\n",
    "* Why do you care about it?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "487dfc9301a51362",
   "metadata": {},
   "source": [
    "Family tree\n",
    "* Nodes: people\n",
    "* It is very large (?). It depends on how much we go back in time\n",
    "* Yes, it can be mapped out.\n",
    "* Yes, it evolves every time someone has a child or gets married\n",
    "* DNA spreading occuers between parent and child nodes.\n",
    "* I care about it because it is interesting to see where your family originally came from. It is also interesting to see where your family members were during the time of big historical events."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9bba782b48ffb25",
   "metadata": {},
   "source": [
    "### In your view what would be the area where network science could have the biggest impact in the next decade? Explain your answer - and base it on the text in the book."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e26dd6fe1df39ba3",
   "metadata": {},
   "source": [
    "In my opinion network science can have the biggest impact in the area of Graph Neural Networks, where we try to train neural networks on graph data structures."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c1b45f47a5eb0cd",
   "metadata": {},
   "source": [
    "---\n",
    "# Exercises 3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d92b87edc447f9e0",
   "metadata": {},
   "source": [
    "### Go to the `NetworkX` project's [tutorial page](https://networkx.github.io/documentation/stable/tutorial.html). The goal of this exercise is to create your own Notebook that contains the entire tutorial. You're free to add your own (e.g. shorter) comments in place of the ones in the official tutorial - and change the code to make it your own where ever it makes sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1e463571-a1dd-4803-a95f-cc32cb0d7f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "G = nx.Graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c7d1230a-937d-48b2-b0c4-74e489c384d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "G.add_node(1)\n",
    "G.add_nodes_from([2, 3])\n",
    "G.add_nodes_from([(4, {\"color\": \"red\"}), (5, {\"color\": \"green\"})])\n",
    "H = nx.path_graph(10)\n",
    "G.add_nodes_from(H)\n",
    "G.add_node(H)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4bff8379-c3c0-4bdb-a707-4cce55957cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "G.add_edge(1, 2)\n",
    "e = (2, 3)\n",
    "G.add_edge(*e)  # unpack edge tuple*\n",
    "G.add_edges_from([(1, 2), (1, 3)])\n",
    "G.add_edges_from(H.edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2f119ae4-aba1-4778-9102-0f7b3a406714",
   "metadata": {},
   "outputs": [],
   "source": [
    "G.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b30109c3-503a-4dbc-a9af-4393f65d51f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "G.add_edges_from([(1, 2), (1, 3)])\n",
    "G.add_node(1)\n",
    "G.add_edge(1, 2)\n",
    "G.add_node(\"spam\")        # adds node \"spam\"\n",
    "G.add_nodes_from(\"spam\")  # adds 4 nodes: 's', 'p', 'a', 'm'\n",
    "G.add_edge(3, 'm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5dc4d918-1f95-48e8-8151-fa4ef4a44d9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "G.number_of_nodes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a099fa48-4d85-4025-8b89-e601fe2ad82f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "G.number_of_edges()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3fbff08a-bb48-4dc2-91a7-0473761fd71e",
   "metadata": {},
   "outputs": [],
   "source": [
    "DG = nx.DiGraph()\n",
    "DG.add_edge(2, 1)   # adds the nodes in order 2, 1\n",
    "DG.add_edge(1, 3)\n",
    "DG.add_edge(2, 4)\n",
    "DG.add_edge(1, 2)\n",
    "assert list(DG.successors(2)) == [1, 4]\n",
    "assert list(DG.edges) == [(2, 1), (2, 4), (1, 3), (1, 2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1df109dd-5af9-4d36-923c-f44dff1f5fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(G.nodes)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
