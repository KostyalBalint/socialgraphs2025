{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Week 8\n",
    "## Overview\n",
    "\n",
    "It's the last time we meet in class for exercises! And to celebrate this mile-stone, I've put together an very nice little set of exercises. And if you're behind, don't worry. The workload is low!\n",
    "\n",
    "  - Part A: First, we play around with sentiment analysis\n",
    "  - That's it!\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Part A: Sentiment analysis"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Sentiment analysis is another highly useful technique which we'll use to make sense of the Wiki\n",
    "data. Further, experience shows that it might well be very useful when you get to the project stage of the class."
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "\n",
    "> **Video Lecture**: Uncle Sune talks about sentiment and his own youthful adventures.\n",
    "\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import nltk\n",
    "from IPython.display import YouTubeVideo\n",
    "YouTubeVideo(\"JuYcaYYlfrI\",width=800, height=450)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# There's also this one from 2010 with young Sune's research\n",
    "YouTubeVideo(\"hY0UCD5UiiY\",width=800, height=450)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "> Reading: [Temporal Patterns of Happiness and Information in a Global Social Network: Hedonometrics and Twitter](http://journals.plos.org/plosone/article?id=10.1371/journal.pone.0026752)\n",
    ">\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "*Exercise*: Sentiment distribution.\n",
    ">\n",
    "> * Download the LabMT wordlist. It's available as supplementary material from [Temporal Patterns of Happiness and Information in a Global Social Network: Hedonometrics and Twitter](http://journals.plos.org/plosone/article?id=10.1371/journal.pone.0026752) (Data Set S1). Describe briefly how the list was generated.\n",
    "> * Based on the LabMT word list, write a function that calculates sentiment given a list of tokens (the tokens should be lower case, etc).\n",
    "> * Iterage over the nodes in your network, tokenize each page, and calculate sentiment every single page. Now you have sentiment as a new nodal property.\n",
    "> * Calculate the average sentiment across all the pages. Also calculate the median, variance, 25th percentile, 75th percentile.\n",
    "> * Remember histograms? Create a histogram of all of the artists's associated page-sentiments. (And make it a nice histogram - use your histogram making skills from Week 2). Add the mean, meadian, ect from above to your plot.\n",
    "> * Who are the 10 artists with happiest and saddest pages?\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "As long as you get the plots right, it's OK to use LLM help here.\n",
    "</div>\n",
    "\n",
    "*Exercise*: Community sentiment distribution. \n",
    "  \n",
    "> * Last week we calculated the stuctural communities of the graph. For this exercise, we use those communities (just the 10 largest ones). Specifically, you should calculate the average the average sentiment of the nodes in each community to find a *community level sentiment*. \n",
    ">   - Name each community by its three most connected bands. (Or feed the list of bands in each community and ask the LLM to come up with a good name for the community).\n",
    ">   - What are the three happiest communities? \n",
    ">   - what are the three saddest communities?\n",
    ">   - Do these results confirm what you can learn about each community by comparing to the genres, checking out the word-clouds for each community, and reading the wiki-pages? \n",
    "> * Compare the sentiment of the happiest and saddest communities to the overall (entire network) distribution of sentiment that you calculated in the previous exercise. Are the communities very differenct from the average? Or do you find the sentiment to be quite similar across all of the communities?\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "As above, feel free to go nuts with help from an LLM with this exercise for the technical parts. But try to answer the questions about interpreting the results with your own human brain.\n",
    "</div>\n",
    "\n",
    "**Note**: Calculating sentiment may take a long time, so arm yourself with patience as your code runs (remember to check that it runs correctly, before waiting patiently). Further, these tips may speed things up. And save somewhere, so you don't have to start over.\n",
    "\n",
    "**Tips for speed**\n",
    "* If you use `freqDist` prior to finding the sentiment, you only have to find it for every unique word and hereafter you can do a weighted mean.\n",
    "* More tips for speeding up loops https://wiki.python.org/moin/PythonSpeed/PerformanceTips#Loops"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "word_scores = {}\n",
    "with open(\"labMIT-1.0.txt\", 'r', encoding='utf-8') as f:\n",
    "            next(f)\n",
    "            for line in f:\n",
    "                parts = line.strip().split('\\t')\n",
    "                if len(parts) >= 3:\n",
    "                    word = parts[0].lower()\n",
    "                    happiness_score = float(parts[2])\n",
    "                    word_scores[word] = happiness_score"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "word_scores"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('averaged_perceptron_tagger_eng')"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import statistics\n",
    "import nltk\n",
    "\n",
    "\n",
    "\n",
    "def calculate_sentiment(text):\n",
    "    scores = []\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    total_tokens = len(tokens)\n",
    "    scored_tokens = 0\n",
    "\n",
    "    # Collect all scores\n",
    "    for token in tokens:\n",
    "        token_lower = token.lower()\n",
    "        if token_lower in word_scores:\n",
    "            scored_tokens += 1\n",
    "            score = word_scores[token_lower]\n",
    "\n",
    "\n",
    "            scores.append(score)\n",
    "\n",
    "    used_tokens = len(scores)\n",
    "\n",
    "    # Initialize result dictionary\n",
    "    result = {\n",
    "        'scores': scores\n",
    "    }\n",
    "\n",
    "    # Calculate statistics if we have scores\n",
    "    if scores:\n",
    "        result['mean'] = statistics.mean(scores)\n",
    "        result['median'] = statistics.median(scores)\n",
    "        result['variance'] = statistics.variance(scores) if len(scores) > 1 else 0.0\n",
    "\n",
    "        sorted_scores = sorted(scores)\n",
    "        result['percentile_25'] = statistics.quantiles(sorted_scores, n=4)[0] if len(scores) >= 2 else sorted_scores[0]\n",
    "        result['percentile_75'] = statistics.quantiles(sorted_scores, n=4)[2] if len(scores) >= 2 else sorted_scores[0]\n",
    "    else:\n",
    "        # No scores available\n",
    "        result['mean'] = None\n",
    "        result['median'] = None\n",
    "        result['variance'] = None\n",
    "\n",
    "        result['percentile_25'] = None\n",
    "        result['percentile_75'] = None\n",
    "\n",
    "    return result"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import json\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "def extract_wiki_text(data):\n",
    "    pages = data[\"query\"][\"pages\"]\n",
    "    first_page = next(iter(pages.values()))\n",
    "    return first_page[\"revisions\"][0][\"slots\"][\"main\"][\"*\"]\n",
    "\n",
    "#We made a function that returns the  words in a json file based on the regex as below\n",
    "def get_wiki_text(current_artist_file_name):\n",
    "    file_path = Path(current_artist_file_name)\n",
    "    with file_path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    text = extract_wiki_text(data)\n",
    "\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "\n",
    "    #words = re.findall(r\"\\b[\\wâ€™-]+\\b\", text, flags=re.UNICODE) #if we need a list of the words\n",
    "\n",
    "    return text"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "get_wiki_text(\"wikipedia_pages/10cc.json\")"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import os\n",
    "\n",
    "sentiment_scores_of_pages = {}\n",
    "\n",
    "items = os.listdir(\"wikipedia_pages\")\n",
    "files = [f for f in items if os.path.isfile(os.path.join(\"wikipedia_pages\", f))]\n",
    "print(f\"Found {len(files)} files:\")\n",
    "for file in files:\n",
    "    print(file)\n",
    "    if file.endswith(\".json\"):\n",
    "        text = get_wiki_text(\"wikipedia_pages/\" + file)\n",
    "        sentiment = calculate_sentiment(text)['mean']\n",
    "        sentiment_scores_of_pages[file] = sentiment\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "sentiment_scores_of_pages"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "data = list(sentiment_scores_of_pages.values())"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(data, bins=30, edgecolor='black', alpha=0.7, color='skyblue')\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Value', fontsize=12)\n",
    "plt.ylabel('Frequency', fontsize=12)\n",
    "plt.title('Histogram of Values', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Add grid for better readability\n",
    "plt.grid(axis='y', alpha=0.3, linestyle='--')\n",
    "\n",
    "# Display statistics on the plot\n",
    "mean_val = np.mean(data)\n",
    "median_val = np.median(data)\n",
    "plt.axvline(mean_val, color='red', linestyle='--', linewidth=2, label=f'Mean: {mean_val:.2f}')\n",
    "plt.axvline(median_val, color='green', linestyle='--', linewidth=2, label=f'Median: {median_val:.2f}')\n",
    "plt.legend()"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "happiest_10 = sorted(sentiment_scores_of_pages.items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "\n",
    "# Get 10 smallest\n",
    "saddest_10 = sorted(sentiment_scores_of_pages.items(), key=lambda x: x[1])[:10]\n",
    "\n",
    "print(\"10 Happiest:\")\n",
    "for key, value in happiest_10:\n",
    "    print(f\"{key}: {value}\")\n",
    "\n",
    "print(\"\\n10 Saddest:\")\n",
    "for key, value in saddest_10:\n",
    "    print(f\"{key}: {value}\")"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": ""
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
