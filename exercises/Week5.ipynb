{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 5\n",
    "## Overview\n",
    "\n",
    "This week we'll talk about advanced network measures (beyond the degree distribution). We will use these tools to make sense of the network of the Rock Musicians on Wikipedia. \n",
    "This is the outline for today:\n",
    "\n",
    "* Visualization and qualitative analysis of the Rock Music Artists Network\n",
    "* Properties of the network structure\n",
    "* Finally some more visualizations\n",
    "\n",
    "> _Reading_: This week, the reading is mostly for reference. It's for you to have a place to go, if you want more detailed information about the topics that I cover in the video lectures. Thus, I recommend you check out **Chapter 9** of the network science book. In particular, we'll delve into Section 9.4 in the exercises below. We will also talk a little bit about degree correlations - you can read about those in **Chapter 7**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part A: Let's  visualise some more!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the rest of the course, we will primarily use[`ForceAtlas2`](https://journals.plos.org/plosone/article%3Fid=10.1371/journal.pone.0098679), a layout algorithm which was developed by the creators of the [Gephy graph analysis software](https://gephi.org). IMHO it's the prettiest network layout algorithm & since last year it's been implemented in NetworkX, so it should be easy to use.\n",
    "\n",
    "*Exercise 1:* Plot the Network using ForceAtlas2. For visualization, you should work with the network from last time, the Giant Connected Component ... and the undirected version.\n",
    "\n",
    "> * Visualize the network using NetworkX\n",
    ">   * Node color should depend on the `length_of_content` attribute (see notes from last week).\n",
    ">   * Node size should depend on the node *degree*\n",
    "> * Play with the force atlas algorithm parameters to obtain a visualization you like. \n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "It's OK to use your LLM as much as needed for this exercise. The only goal is to create a nice visualization.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-28T20:03:54.561306Z",
     "start_time": "2025-10-28T20:03:53.885344Z"
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "from urllib.parse import quote\n",
    "import time\n",
    "import json\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import re\n",
    "#We made a wikipediadownloader class that downloads and saves the json version of a wikipedia page\n",
    "\n",
    "class WikipediaDownloader:\n",
    "    def __init__(self):\n",
    "        self.base_url = \"https://en.wikipedia.org/w/api.php\"\n",
    "\n",
    "        self.session = requests.Session()\n",
    "        self.session.headers.update({\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "        })\n",
    "\n",
    "    def download_page_html(self, title):\n",
    "        try:\n",
    "            params = {\n",
    "                \"action\": \"query\",\n",
    "                \"prop\": \"revisions\",\n",
    "                \"rvprop\": \"content\",\n",
    "                \"format\": \"json\",\n",
    "                \"titles\": title.replace(' ', '_'),  # Use the actual title parameter\n",
    "                \"rvslots\": \"main\"\n",
    "            }\n",
    "            print(f\"Downloading: {title}\")\n",
    "            response = self.session.get(self.base_url, params=params)\n",
    "\n",
    "            response.raise_for_status()\n",
    "            return response.text\n",
    "        except requests.RequestException as e:\n",
    "            print(f\"Error downloading {title}: {e}\")\n",
    "            return None\n",
    "\n",
    "    def save_page(self, title, content):\n",
    "        if content is None:\n",
    "            return False\n",
    "\n",
    "        # Create directory if it doesn't exist\n",
    "        os.makedirs(\"wikipedia_pages\", exist_ok=True)\n",
    "\n",
    "        # Clean filename\n",
    "        safe_filename = \"\".join(c for c in title if c.isalnum() or c in (' ', '-', '_')).rstrip()\n",
    "        filename = f\"wikipedia_pages/{safe_filename}.json\"\n",
    "\n",
    "        try:\n",
    "            with open(filename, 'w', encoding='utf-8') as f:\n",
    "                f.write(content)\n",
    "            print(f\"Saved: {filename}\")\n",
    "            return filename\n",
    "        except IOError as e:\n",
    "            print(f\"Error saving {title}: {e}\")\n",
    "            return False\n",
    "\n",
    "    def download_and_save(self, title):\n",
    "        content = self.download_page_html(title)\n",
    "\n",
    "        if content:\n",
    "            return self.save_page(title, content)\n",
    "        return False\n",
    "\n",
    "downloader = WikipediaDownloader()\n",
    "\n",
    "favorite_pages = [\n",
    "    \"List of mainstream rock performers\"\n",
    "]\n",
    "for page_title in favorite_pages:\n",
    "    print(f\"\\n--- Processing: {page_title} ---\")\n",
    "\n",
    "    # Download HTML version\n",
    "    file_name = downloader.download_and_save(page_title)\n",
    "    if(file_name):\n",
    "        print(f\"Successfully downloaded and saved: {file_name}\")\n",
    "    else:\n",
    "        print(f\"Failed to download or save: {page_title}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "nx.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-28T20:03:54.575569Z",
     "start_time": "2025-10-28T20:03:54.572589Z"
    }
   },
   "outputs": [],
   "source": [
    "#We extracted the artists from the mainstreem rock performers wiki\n",
    "import json\n",
    "with open(\"wikipedia_pages/List of mainstream rock performers.json\", \"r\") as f:\n",
    "    html_content = f.read()\n",
    "\n",
    "#we had to decode the json because of the special characters\n",
    "parsed_json = json.loads(html_content)\n",
    "page_id = list(parsed_json['query']['pages'].keys())[0]\n",
    "wiki_text = parsed_json['query']['pages'][page_id]['revisions'][0]['slots']['main']['*']\n",
    "\n",
    "# Now extract from the properly decoded text\n",
    "pattern = r'\\[\\[([^\\]|]+)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-28T20:04:01.976068Z",
     "start_time": "2025-10-28T20:04:01.972372Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "import re\n",
    "# Find the artists and saved them to the artists.txt\n",
    "matches = re.findall(pattern, wiki_text)\n",
    "\n",
    "# Optional: remove duplicates and sort\n",
    "artists = sorted(set(matches))\n",
    "\n",
    "# Save to a text file\n",
    "with open(\"artists.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for artist in artists:\n",
    "        f.write(artist + \"\\n\")\n",
    "\n",
    "print(f\"Extracted {len(artists)} artists and saved to artists.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-28T20:08:56.311111Z",
     "start_time": "2025-10-28T20:04:15.461195Z"
    }
   },
   "outputs": [],
   "source": [
    "# Open the file in read mode\n",
    "artists_file_names = []\n",
    "with open(\"artists.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    for line in file:\n",
    "        artist = line.strip()  # remove leading/trailing spaces and newline\n",
    "        file_name = downloader.download_and_save(artist)\n",
    "        if(file_name):\n",
    "            artists_file_names.append(file_name)\n",
    "        else:\n",
    "            print(f\"Failed to download or save: {page_title}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-29T11:48:07.353455Z",
     "start_time": "2025-10-29T11:48:07.348087Z"
    }
   },
   "outputs": [],
   "source": [
    "#we saved the artists to an artist_array so that it will be easier to iterate through them later\n",
    "artist_array = []\n",
    "with open(\"artists.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    for line in file:\n",
    "        artist_array.append(line.strip())  # remove leading/trailing spaces and newline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-29T11:48:09.090125Z",
     "start_time": "2025-10-29T11:48:09.087486Z"
    }
   },
   "outputs": [],
   "source": [
    "# We made a function that checks if an artist is referenced in another artist's wikipedia page\n",
    "def find_referenced_artists(artist_to_find, current_artist, current_artist_file_name):\n",
    "    file_path = current_artist_file_name\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        wiki_html = f.read()\n",
    "\n",
    "    # Find all wiki links with regex\n",
    "    pattern = r\"\\[\\[\" + re.escape(artist_to_find) + r\"(?:\\|[^\\]]+)?\\]\\]\"\n",
    "\n",
    "    if re.search(pattern, wiki_html):\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-29T11:48:36.008744Z",
     "start_time": "2025-10-29T11:48:24.147141Z"
    }
   },
   "outputs": [],
   "source": [
    "#We made a directed graph and added the artists as nodes and if an artist referenced another artist in his wikipedia page we added a directed edge from the first artist to the second artist.\n",
    "import networkx as nx\n",
    "rock_artists_graph = nx.DiGraph()\n",
    "\n",
    "for artist in artist_array:\n",
    "    rock_artists_graph.add_node(artist)\n",
    "\n",
    "for index in range (len(artist_array)):\n",
    "    artist = artist_array[index]\n",
    "    artist_file_name = artists_file_names[index]\n",
    "    for artist_to_find in artist_array:\n",
    "        if artist != artist_to_find:\n",
    "            if find_referenced_artists(artist_to_find, artist, artist_file_name):\n",
    "                rock_artists_graph.add_edge(artist, artist_to_find)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-29T11:48:37.849782Z",
     "start_time": "2025-10-29T11:48:37.847735Z"
    }
   },
   "outputs": [],
   "source": [
    "#after analysing the data, we found a couple of strings that needed to be removed\n",
    "rock_artists_graph.remove_node('Category:Lists of rock musicians')\n",
    "rock_artists_graph.remove_node('Category:Lists of rock musicians by subgenre')\n",
    "rock_artists_graph.remove_node('AllMusic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-29T14:20:36.175337Z",
     "start_time": "2025-10-29T14:20:36.163466Z"
    }
   },
   "outputs": [],
   "source": [
    "#We made a helper function that extracts the text from the Wikipedia json structure\n",
    "#We used llm to help us with this 2 functions\n",
    "def extract_wiki_text(data):\n",
    "    pages = data[\"query\"][\"pages\"]\n",
    "    first_page = next(iter(pages.values()))\n",
    "    return first_page[\"revisions\"][0][\"slots\"][\"main\"][\"*\"]\n",
    "\n",
    "#We made a function that returns the number of words in a json file based on the regex as below\n",
    "def word_count(current_artist_file_name):\n",
    "    file_path = Path(current_artist_file_name)\n",
    "    with file_path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    text = extract_wiki_text(data)\n",
    "\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "\n",
    "    words = re.findall(r\"\\b[\\wâ€™-]+\\b\", text, flags=re.UNICODE)\n",
    "\n",
    "    return len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-29T14:21:07.176243Z",
     "start_time": "2025-10-29T14:21:05.524509Z"
    }
   },
   "outputs": [],
   "source": [
    "#We connect the artists_file_names with the artist_arrays to create a dictionary\n",
    "artists_to_path = dict(zip((artist_array), (artists_file_names)))\n",
    "\n",
    "word_count_array = []\n",
    "for artist in rock_artists_graph.nodes():\n",
    "    file_path = artists_to_path[artist]\n",
    "    word_num = word_count(file_path)\n",
    "    word_count_array.append(word_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-29T14:21:29.893094Z",
     "start_time": "2025-10-29T14:21:29.889954Z"
    }
   },
   "outputs": [],
   "source": [
    "artists_to_dict = {}\n",
    "for index, item in enumerate(list(rock_artists_graph.nodes.items())):\n",
    "    artists_to_dict[item[0]] = word_count_array[index]\n",
    "\n",
    "nx.set_node_attributes(rock_artists_graph, values = artists_to_dict, name = \"word_count\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-29T14:22:36.366890Z",
     "start_time": "2025-10-29T14:22:36.363732Z"
    }
   },
   "outputs": [],
   "source": [
    "#We made a function that disconnect nodes that don't have any edges\n",
    "def disconnect_node(graph_name):\n",
    "    nodes_with_no_out_degree = []\n",
    "    for node in graph_name:\n",
    "        if graph_name.out_degree(node) == 0 and graph_name.in_degree(node) == 0:\n",
    "            nodes_with_no_out_degree.append(node)\n",
    "\n",
    "    graph_name.remove_nodes_from(nodes_with_no_out_degree)\n",
    "    disconnected = nodes_with_no_out_degree\n",
    "    return disconnected\n",
    "\n",
    "disconnect = disconnect_node(rock_artists_graph)\n",
    "print(f\"Disconnected nodes: \", disconnect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-29T14:22:37.165308Z",
     "start_time": "2025-10-29T14:22:37.156203Z"
    }
   },
   "outputs": [],
   "source": [
    "#We search for the largest component\n",
    "largest_component_sort = sorted(nx.weakly_connected_components(rock_artists_graph), key=len, reverse=True)\n",
    "largest_component = rock_artists_graph.subgraph(largest_component_sort[0])\n",
    "\n",
    "# add who is the largest component with number of edges and nodes\n",
    "\n",
    "largest_component_nodes = largest_component.number_of_nodes()\n",
    "print(f\"The number of nodes of the largest component is: \", largest_component_nodes)\n",
    "\n",
    "#number of links\n",
    "largest_component_edges = largest_component.number_of_edges()\n",
    "\n",
    "print(\"The number of links in the largest component is:\", largest_component_edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-29T14:22:38.702011Z",
     "start_time": "2025-10-29T14:22:38.696724Z"
    }
   },
   "outputs": [],
   "source": [
    "#import pickle\n",
    "#with open(\"../files/rock_artists.p\", 'wb') as f:\n",
    "#    pickle.dump(rock_artists_graph, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-29T14:22:40.105937Z",
     "start_time": "2025-10-29T14:22:40.100871Z"
    }
   },
   "outputs": [],
   "source": [
    "#load the rock artists graph\n",
    "with open(\"../files/rock_artists.p\", 'rb') as f:  # notice the r instead of w\n",
    "    rock_artists_graph = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-29T14:25:45.734722Z",
     "start_time": "2025-10-29T14:25:45.077354Z"
    }
   },
   "outputs": [],
   "source": [
    "nx.forceatlas2_layout(rock_artists_graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Exercise 2*: We continue with a qualitative inspection of the Wikipedia pages. Inspecting the data manually will develop intuitions that will help us to intepret our quantitative results. We investigate the reasons for links between characters to exist on Wikipedia.\n",
    "\n",
    "> 1. Consider the nodes with highest in-degree in the network. Open the corresponding Wikipedia page.\n",
    ">    * Consider the set of neighbours pointing towards the node found above. \n",
    ">    * What is the relation between the band/mucisians you identified and their network neighbours? (e.g. did they sing a song together, did one write a song for the other?, etc ...). You don't need to come up with a grand theory of Rock Music here, I just want you to think about how links arise, why the links are there.\n",
    ">    * Repeat the exercise for the highest out-degree node. \n",
    ">    * Are there differences between reasons for out- and in-links? Again, there's no right answer here. I just want you to think about some of the reasons why links may occur in Wikipedia. Answer in your own words.\n",
    "> 4. Create a scatter plot, where each node is a point, and the axes show in- versus out-degree. That way you can visually inspect the patterns of how in- and out-degree are connected across all the nodes in the network. Comment on what you see.\n",
    "> 5. Are there any nodes with high(ish) out-degree and with few incoming connections? If yes, open WikiPages of those bands/mucisians and explain what happens?\n",
    "> 6. Are there any nodes with high in-degree and few going connections? Again, open the WikiPages corresponding to those nodes and explain what happens?\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "For this one, I hope you'll limit your LLM use. It's OK to get help in creating the scatter plot, but use NetworkX on your own to identify nodes, and do your own reading and interpreting of Wiki pages.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part B:  Advanced tools \n",
    "\n",
    "> **Video lecture**: *Network measures*. There's an updated video below, and if you can't get enough, you can watch the original version from 2015 [here](https://www.youtube.com/watch?v=0uAYpOqVHCU)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import YouTubeVideo\n",
    "YouTubeVideo(\"IOWXZFOyk9Y\", width=800, height=450)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Exercise 3:* Last time we visually analyzed (plotted) the degree distributions of our network. Now, let's analyze it using some of the tools from previous lectures. For this exercise we are going to look at the *directed* graph (rememeber, we asked you to keep one?).\n",
    "\n",
    "> 1. What are the average, median, mode, minimum and maximum value of the in-degree? And of the out-degree? How do you intepret the results?\n",
    "> 2. Let's also look at the *[exponent of the degree distribution](http://networksciencebook.com/chapter/4#degree-exponent)* \n",
    ">    * To calculate it, you can install [`powerlaw`](https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0085777) package (I could not install in conda, so I used `pip install powerlaw`)\n",
    ">    * Use `powerlaw.Fit(degree).alpha` for fitting\n",
    ">    * Find the degree exponent for in-, out- and total- degree distribution. Think about what you learned in the book - what does each exponent say about our network?\n",
    ">    * When analyzing the plots of in- and out- degree distribution last week, did you come up to the same conclusions?\n",
    ">    * Reflect on the whole power-law thing. Are the degree distributions even power laws? Could a log-normal or exponential distribution be better? How would you find out what the right function to describe the various degree distributions are? \n",
    ">    * A long time ago, a group of scientists analyzed the network spanned by **all** of Wikipedia (the paper is called [Wikipedias: Collaborative web-based encyclopedias as complex networks](https://journals.aps.org/pre/abstract/10.1103/PhysRevE.74.016115); to access this paper, use findit.dtu.dk or read the preprint on [arXiv](https://arxiv.org/abs/physics/0602149)). Do your findings correspond to the results described in Chapter **III.A** of that paper? If not, try to give your explanation of why? \n",
    "> 3. Plot a heatmap [(Hint here)](https://stackoverflow.com/questions/2369492/generate-a-heatmap-in-matplotlib-using-a-scatter-data-set) of the the in- versus out- degree for all characters. Zoom in on just the in-degree $[0,20]$ and out-degree $[0,20]$ area of the plot. What is the advantage of using a heatmap over the scatter plot from the previous Exercise (especially for understanding what's going on in this area)?\n",
    "> 4. Why do you think I want you guys to use a directed graph? Are there questions we can ask using the directed graph that are not possible to answer in the undirected version? Which questions? Can you think of examples when the in/out distinction doesn't matter.\n",
    "> 5. We have one more metric that we have used, but not investigated so far. The node-attribute `length_of_content`. There are two things to look at.\n",
    ">    * Firstly, we can verify that more text means more links. If that's true the length should correlate with out-degree. Does it? \n",
    ">    * Secondly, and more interestingly, it could be that longer texts means *more famous* characters. And that means more in-links. \n",
    ">    * How should you test this? Well, here you are allowed to be **creative**. You may choose the method of your liking to study this question (e.g. look at it visually, test the correlation using [spearman](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.spearmanr.html) or [pearson](https://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.stats.pearsonr.html) correlations, fit a curve to the data, you decide). Are both questions true? If yes, which phenomenon is stronger? Explain your results in your own words.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "For this exercise, I also hope you'll limit your LLM use. Especially for the intrepretation parts. It's OK to get help for the heatmap in sub-part 3, maybe also for ideas of how to do the testing in part 5. But do your own interpretation :)\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Exercise 4:* Centralities, assortativity and more.\n",
    "\n",
    "Earlier, we found the most connected nodes (using degree centrality). \n",
    "\n",
    "Now let's dig in and try to understand more about the network using more advanced features. \n",
    "\n",
    "**Note**: Not all of the measures we'll be considering below are defined for directed graphs. Only use the undirected graph when explicitly stated in the exercise. \n",
    "> 1. Find the 5 most central characters according to degree centrality. \n",
    "> 1. Find the 5 most central characters according to betweenness centrality. \n",
    ">    * Repeat Exercise 1 (plotting the network). However, this time use *betweenness centrality* to define the size of the node.\n",
    ">    * What role do you imagine characters with high wikipedia graph betweenness centrality play in the network? \n",
    "> 2. Find the 5 most central characters according to eigenvector centrality. Calculate centrality corresponding to both in- and out-edges (see NetworkX documentation for details). \n",
    ">    * Repeat Exercise 1 one final time (depending on which visualization method you liked the most). However, this time use eigenvector centrality to define the size of the node.\n",
    ">    * What role do you imagine characters with high wikipedia graph eigenvector centrality play? Describe what is different about betweenness centrality and eigenvector centrality using your visualizations as examples.\n",
    "> 3. Plot the betweenness centrality of nodes vs their degree centrality. Is there a correlation between the two? Did you expect that? Why? \n",
    ">     * Repeat the scatter plot using eigenvector centrality instead of betweenness centrality. Do you observe any difference relative to above? Why?\n",
    "> 4. Is the undirected version of the graph [assortative](https://en.wikipedia.org/wiki/Assortativity) with respect to degree? (e.g. do high-degree characters tend to link to other high-degree characters, and low-degree characters to other low-degree characters?). Provide an interpretation of your answer.\n",
    "> 5. Is the undirected version of the graph [assortative](https://en.wikipedia.org/wiki/Assortativity) with respect do the `length_of_content` nodal properties? (e.g. do characters with long Wikipedia pages tend to link to other charakters with long Wiki-pages, and *vice versa*?.\n",
    ">      * Hint: Use [this function](https://networkx.github.io/documentation/stable/reference/algorithms/generated/networkx.algorithms.assortativity.attribute_assortativity_coefficient.html#networkx.algorithms.assortativity.attribute_assortativity_coefficient). Provide an interpretation of your answer.\n",
    "> 6. What is the average shortest path length? Is it similar to the one of a random graph with the same number of nodes $N$  and probablity of connection $p$?\n",
    ">    * Take a look at the *distribution* of shortest paths for the two networks (create a histogram for each network) to better compare and contrast. Explain what you see in your own words.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "As in the two previous exercises, I hope you'll limit your LLM use. Try to get a sense of working with NetworkX. And to grow your brain, do your own intrepretation parts ... it's difficult, but that's how you grow.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Eni part"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\nThe neighbours of the top in_degree artists are:\")\n",
    "\n",
    "top5_in_degree = sorted(rock_artists_graph.in_degree(), key=lambda x: x[1], reverse=True)[:5]\n",
    "\n",
    "top_artist_in_degree = top5_in_degree[0]\n",
    "top_artist_in_neighbours = list(rock_artists_graph.neighbors(top_artist_in_degree[0]))\n",
    "for i in top_artist_in_neighbours:\n",
    "    print(i)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\nThe neighbours of the top in_degree artists are:\")\n",
    "top5_out_degree = sorted(rock_artists_graph.out_degree(), key=lambda x: x[1], reverse=True)[:5]\n",
    "\n",
    "top_artist_out_degree = top5_out_degree[0]\n",
    "print(f\"The top artist with the most out degree: \", top_artist_out_degree[0])\n",
    "top_artist_out_neighbours = list(rock_artists_graph.neighbors(top_artist_out_degree[0]))\n",
    "for i in top_artist_out_neighbours:\n",
    "    print(i)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rock_artists_graph_attributes ={\"graph\":rock_artists_graph, \"in_degrees\":[], \"out_degrees\":[], \"min_degree\":0, \"max_degree\":0}\n",
    "\n",
    "scale_free_graph_attributes = {\"graph\":nx.scale_free_graph(rock_artists_graph.number_of_nodes()), \"in_degrees\":[], \"out_degrees\":[], \"min_degree\":0, \"max_degree\":0}\n",
    "\n",
    "random_graph_attributes = {\"graph\":nx.gnm_random_graph(rock_artists_graph.number_of_nodes(), rock_artists_graph.number_of_edges(), directed=True), \"in_degrees\":[], \"out_degrees\":[], \"min_degree\":0, \"max_degree\":0}\n",
    "\n",
    "\n",
    "graphs = [rock_artists_graph_attributes, scale_free_graph_attributes, random_graph_attributes]\n",
    "\n",
    "for graph in graphs:\n",
    "    in_degrees_array = []\n",
    "    out_degrees_array = []\n",
    "    for node in graph[\"graph\"].nodes():\n",
    "        in_degree = graph[\"graph\"].in_degree(node)\n",
    "        out_degree = graph[\"graph\"].out_degree(node)\n",
    "        in_degrees_array.append(in_degree)\n",
    "        out_degrees_array.append(out_degree)\n",
    "    graph[\"in_degrees\"] = in_degrees_array\n",
    "    graph[\"out_degrees\"] = out_degrees_array\n",
    "    degrees = dict(graph[\"graph\"].degree())\n",
    "    graph[\"min_degree\"] = min(degrees.values())\n",
    "    graph[\"max_degree\"] = max(degrees.values())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "x_values = rock_artists_graph_attributes[\"in_degrees\"]\n",
    "y_values = rock_artists_graph_attributes[\"out_degrees\"]\n",
    "\n",
    "plt.scatter(x_values, y_values)\n",
    "plt.xlabel(\"In-Degrees\")\n",
    "plt.ylabel(\"Out-Degrees\")\n",
    "plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find node with low out-degree but with many in-degrees\n",
    "\n",
    "in_deg = dict(rock_artists_graph.in_degree())\n",
    "out_deg = dict(rock_artists_graph.out_degree())\n",
    "\n",
    "# find nodes with high out-degree but low in-degree\n",
    "threshold_out = np.percentile(list(out_deg.values()), 90)  # top 10% out-degree\n",
    "threshold_in = np.percentile(list(in_deg.values()), 30)   # bottom 30% in-degree\n",
    "\n",
    "interesting_nodes = [\n",
    "    node for node in rock_artists_graph.nodes()\n",
    "    if out_deg[node] >= threshold_out and in_deg[node] <= threshold_in\n",
    "]\n",
    "\n",
    "print(\"Artists with high out-degree but few incoming connections:\")\n",
    "for node in interesting_nodes:\n",
    "    print(f\"{node}: in={in_deg[node]}, out={out_deg[node]}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find node with many out-degree but with low in-degrees \n",
    "\n",
    "# find nodes with low out-degree but high in-degree\n",
    "threshold_out = np.percentile(list(out_deg.values()), 30)  # top 10% out-degree\n",
    "threshold_in = np.percentile(list(in_deg.values()), 90)   # bottom 30% in-degree\n",
    "\n",
    "interesting_nodes2 = [\n",
    "    node for node in rock_artists_graph.nodes()\n",
    "    if out_deg[node] <= threshold_out and in_deg[node] >= threshold_in\n",
    "]\n",
    "\n",
    "print(\"Artists with high out-degree but few incoming connections:\")\n",
    "for node in interesting_nodes2:\n",
    "    print(f\"{node}: in={in_deg[node]}, out={out_deg[node]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "#average in degree\n",
    "total_in_degree = sum(in_deg.values())\n",
    "avg_in_degree = total_in_degree / rock_artists_graph.number_of_nodes()\n",
    "print(f\"The average in-degree is: \", avg_in_degree)\n",
    "\n",
    "#median in degree\n",
    "median_in_degree = np.median(list(in_deg.values()))\n",
    "print(f\"The median indegree is: \", median_in_degree)\n",
    "\n",
    "#mode in degree\n",
    "mode_in_degree = stats.mode(list(in_deg.values()), keepdims=False).mode\n",
    "print(f\"The mode indegree is: \", mode_in_degree)\n",
    " \n",
    "#minimum in degree\n",
    "min_in_degree = min(in_deg.values())\n",
    "print(f\"The minimum in-degree is: \", min_in_degree)\n",
    "\n",
    "#maximum in degree\n",
    "maximum_in_degree = max(in_deg.values())\n",
    "print(f\"The maximum in-degree is: \", maximum_in_degree)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import powerlaw\n",
    "in_degree_values = list(in_deg.values())\n",
    "in_degree_values = [val for val in in_degree_values if val >0]\n",
    "in_degree_results = powerlaw.Fit(in_degree_values, discrete=True)\n",
    "print (f\"In-degree results: \", in_degree_results.power_law.alpha)\n",
    "\n",
    "in_degree_xmin_value = in_degree_results.power_law.xmin\n",
    "print (f\"Minimum in-degree value: \", in_degree_xmin_value)\n",
    "\n",
    "in_degree_KS_value = in_degree_results.power_law.KS()\n",
    "print (f\"In-degree KS value: \", in_degree_KS_value)\n",
    "\n",
    "\n",
    "out_degree_values = list(out_deg.values())\n",
    "out_degree_values = [val for val in out_degree_values if val >0 ]\n",
    "out_degree_results = powerlaw.Fit(out_degree_values, discrete=True)\n",
    "print (f\"Out-degree results: \", out_degree_results.power_law.alpha)\n",
    "\n",
    "#total degree distribution\n",
    "total_deg = dict(rock_artists_graph.degree())   \n",
    "total_degree_values = list(total_deg.values())\n",
    "total_degree_values = [val for val in total_degree_values if val > 0]  # Remove zero values\n",
    "total_degree_results = powerlaw.Fit(total_degree_values, discrete=True)\n",
    "print (f\"Total degree results: \", total_degree_results.power_law.alpha)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Part B: Advanced tools\n",
    "#3. heatmap \n",
    "x_val = list(in_deg.values())\n",
    "y_val = list(out_deg.values())\n",
    "\n",
    "heatmap, xedges, yedges = np.histogram2d(x_val, y_val, bins=40, range = [[0,20],[0,20]])\n",
    "extent = [xedges[0], xedges[-1], yedges[0], yedges[-1]]\n",
    "\n",
    "plt.clf()\n",
    "plt.xlabel('In-Degrees')\n",
    "plt.ylabel('Out-Degrees')\n",
    "plt.title('Heatmap of In-Degrees vs Out-Degrees')\n",
    "plt.imshow(heatmap.T, extent=extent, origin='lower')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#5 - correlation with length of content\n",
    "\n",
    "import scipy.stats as stats\n",
    "#use the length of content and verify if there is a correlation with the in-degrees and out-degrees\n",
    "word_counts = []\n",
    "for node in rock_artists_graph.nodes():\n",
    "    word_count_value = rock_artists_graph.nodes[node]['word_count']\n",
    "    word_counts.append(word_count_value)\n",
    "\n",
    "correlation_in_degree_length_of_content = stats.pearsonr(list(in_deg.values()), word_counts)\n",
    "correlation_out_degree_length_of_content = stats.pearsonr(list(out_deg.values()), word_counts)\n",
    "\n",
    "correlation_in_degree_length_of_content2 = stats.spearmanr(list(in_deg.values()), word_counts)\n",
    "correlation_out_degree_length_of_content2 = stats.spearmanr(list(out_deg.values()), word_counts)\n",
    "\n",
    "print(f\"Using the Pearson correlation:\\n\")\n",
    "print(f\"The correlation between in-degrees and length of content is: \", correlation_in_degree_length_of_content.statistic)\n",
    "print(f\"The correlation between out-degrees and length of content is: \", correlation_out_degree_length_of_content.statistic)\n",
    "\n",
    "print(f\"\\nUsing the Spearman correlation:\\n\")\n",
    "print(f\"The correlation between in-degrees and length of content is: \", correlation_in_degree_length_of_content2.statistic)\n",
    "print(f\"The correlation between out-degrees and length of content is: \", correlation_out_degree_length_of_content2.statistic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deg_centrality = nx.degree_centrality(rock_artists_graph)\n",
    "top5_deg_centrality = sorted(deg_centrality.items(), key=lambda x: x[1], reverse=True)[:5]\n",
    "\n",
    "print(f\"Top 5 most central asrtist based on degree centrality: \")\n",
    "for node, centrality in top5_deg_centrality:\n",
    "    print(f\"{node}: {centrality}\")\n",
    "\n",
    "betweenness_centrality = nx.betweenness_centrality(rock_artists_graph)\n",
    "top5_betweenness_centrality =  sorted(betweenness_centrality.items(), key=lambda x: x[1], reverse=True)[:5]\n",
    "print(f\"\\nTop 5 most central artist based on betweenness centrality: \")\n",
    "for node, centrality in top5_betweenness_centrality:\n",
    "    print(f\"{node}: {centrality}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#node size based on betweenness centrality\n",
    "\n",
    "betweenness_centrality = nx.betweenness_centrality(rock_artists_graph)\n",
    "node_size_bc = [betweenness_centrality[node] * 10000 for node in rock_artists_graph.nodes()]\n",
    "\n",
    "node_colors = []\n",
    "for node in rock_artists_graph.nodes():\n",
    "    grad_word_count = rock_artists_graph.nodes[node]['word_count']\n",
    "    node_colors.append(grad_word_count)\n",
    "    \n",
    "pos = nx.forceatlas2_layout(rock_artists_graph, gravity = 10)\n",
    "plt.figure(1, figsize=(20,20))\n",
    "nx.draw_networkx(rock_artists_graph, pos, width=0.1, node_size = node_size_bc, node_color = node_colors, font_size= 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find the 5 most central characters according to eigenvector centrality.\n",
    "\n",
    "eigenvector_centrality_out = nx.eigenvector_centrality(rock_artists_graph, max_iter=1000, tol=1e-06)\n",
    "top5_eigenvector_centrality_out = sorted(eigenvector_centrality_out.items(), key=lambda x: x[1], reverse=False)[:5]\n",
    "print(f\"\\nTop 5 most central artist based on eigenvector centrality: \")\n",
    "for node, centrality in top5_eigenvector_centrality_out:\n",
    "    print(f\"{node}: {centrality}\")\n",
    "\n",
    "undirected_rock_artist_graph = rock_artists_graph.to_undirected(reciprocal=False, as_view=True)\n",
    "\n",
    "eigenvector_centrality_in = nx.eigenvector_centrality(undirected_rock_artist_graph, max_iter=1000, tol=1e-06)\n",
    "top5_eigenvector_centrality_in = sorted(eigenvector_centrality_in.items(), key=lambda x: x[1], reverse=True)[:5]\n",
    "print(f\"\\nTop 5 most central artist based on eigenvector centrality (inverted graph): \")\n",
    "for node, centrality in top5_eigenvector_centrality_in:\n",
    "    print(f\"{node}: {centrality}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 2\n",
    "\n",
    "### Part 1: Analyze the network\n",
    "The questions in this part are based on Lecture 5.\n",
    "\n",
    "-  Present an analysis/description of the network of bands/artists using tools from Lecture 5. Imagine that you have been tasked with presenting the important facts about the network to an audience who knows about network science, but doesn't know about this particular network.\n",
    "    - It's OK to also use basic concepts like degree distributions (even though they're from week 4) in your analysis. That way you can make the analysis a standalone, coherent thing.\n",
    "    - I would like you to include concepts like centrality and assortativity in your analysis.\n",
    "    - Use a network backbone in your analysis.\n",
    "    - In addition to standard distribution plots (e.g. degree distributions, etc), your analysis should also include at least one network visualization (but it doesn't have to display the entire network, you can also visualize a network backbone).\n",
    "    - **Note**: As I write above, an important part of the exercise consists is selecting the right elements of the lecture to create a meaningful analysis. So don't solve this part by going exhaustive and just calculating everything you can think of in one massive analysis. Try to focus on using what you've learned to characterize the network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analyze the network\n",
    "- Degree distribution\n",
    "- Centrality and assortavity analysis\n",
    "- Use the network backcone\n",
    "- Visualize the backbone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use basic concepts like degree distributions in analysis\n",
    "\n",
    "from IPython.display import display_markdown\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "def plot_degree_distribution(degrees, title):\n",
    "    import seaborn as sns\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    sns.histplot(degrees, bins=(max(degrees) - min(degrees) + 1), kde=True)\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "plot_degree_distribution(rock_artists_graph_attributes[\"in_degrees\"], \"In-Degree Distribution of Rock Artists Graph\")\n",
    "display(Markdown(\"**Figure 1.1:** *In-Degree Distribution of Rock Artists Graph*\"))\n",
    "\n",
    "plot_degree_distribution(rock_artists_graph_attributes[\"out_degrees\"], \"Out-Degree Distribution of Rock Artists Graph\")\n",
    "display(Markdown(\"**Figure 1.2:** *Out-Degree Distribution of Rock Artists Graph*\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part C: Backbones and Network Visualization \n",
    "\n",
    "One method to clean up the \"hairball\"-looking networks, is known as the [backbone method](https://www.pnas.org/doi/10.1073/pnas.0808904106). Sometimes this method is also called the \"disparity filter\". I explain what it's all about in the video below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "YouTubeVideo(\"GOvsrVulbsg\", width=800, height=450)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are [many backbone extraction algorithms](https://www.michelecoscia.com/?page_id=287), and I've chosen to talk about the *Disparity Filter* here, because it's the oldest and most commonly used method (so I though you should know about it). Plus, it'll probably be the relevant method for your project.\n",
    "\n",
    "This choice, however, presents us with a problem: As I explain in the video, the Disparity Filter method *only works on weighted networks*. But our network does not have edge weights in any kind of natural way (since there's usually just a single link between two wiki pages). \n",
    "\n",
    "\n",
    "So to get rid of unimportant edges, we'll have to try something different. I present you with three possible methods below. For the first two ones, we'll add weights proportional to the [edge betweenness centrality](https://networkx.org/documentation/stable/reference/algorithms/generated/networkx.algorithms.centrality.edge_betweenness_centrality.html). The edge betweenness centrality is just like the node betweenness centrality that you know and love ... only that it's defined for *edges* instead of nodes.\n",
    "\n",
    "We can create two new weighted networks based on the edge betweenness\n",
    " * **Weighted Network 1**: The ***undirected*** Rock Music Network (Giant Connected Component) but with edge weights proportional to the edge betweenness centrality (let's renormalize so that the lowest weight is equal to 1 and the highest weight is equal to 100).\n",
    " * **Weighted Network 2**: The ***undirected*** Rock Music Network  (Giant Connected Component) but with edge weights proportional to 1/(edge weight in Weighted Network 1).\n",
    "\n",
    "The third strategy (resulting in **Weighted network 3**) is to use a method designed for extracting structure from undirected networks: The [**High-Salience Skeleton**](https://www.nature.com/articles/ncomms1847). It's implemented as part of [this backbone extraction package](https://www.michelecoscia.com/?page_id=287)\n",
    "\n",
    "\n",
    "Now we're ready for the exercise.\n",
    "\n",
    "\n",
    "*Exercise 5*: Visualizing the Network Backbone\n",
    "\n",
    "> 1. Extract the network backbone for both Weighted Network 1, 2, and 3. You can implement the disparity filter yourself based on the [scientific paper](https://www.pnas.org/doi/10.1073/pnas.0808904106) or find an implentation on GitHub (there are several ones to choose from ... I like [this implementation](https://www.michelecoscia.com/?page_id=287), which also includes other interesting edge filters). Figure out how your particular backbone extractor works (you may have to inspect the code itself, these packages are not always well documented). Play around with values of $\\alpha$ to get an aestetically pleasing visualization (but don't make the resulting network too sparse).\n",
    "> 2. Plot the resulting three networks using ForceAtlas2, still sizing nodes according to total degree. No need to color them. If your backbone has some disconnected nodes, feel free to not plot them. And remember to calculate a new layout to see how the structure has changed.\n",
    "> 3. Comment on your results\n",
    ">    * Do you see any structures in the networks now? Inspect some nodes to get a sense of what the structures captures in the three networks. Describe your findings in your own words.\n",
    ">    * Weighted network 1, 2, and 3 are very different. In 1 and 2, we have removed edges with low/high edge betweenness centrality respectively and HSH is based on shortest paths. What have the various filters done to the networks? Describe what you see visually, and your explain your thoughts on what we've done to the network structures.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "There's a lot to do today, so knock youself out with your LLM for this exercise, hopefully it can make your life easier (although I hope you'll do a bit of your own thinking, since a lot of complicated conceptual things are going on with the interplay between the backboning and edge betweenness and inverse edge betweenness!)</div>\n",
    "\n",
    "One final thing. For now, the backbones are mostly for visualization. Especially because the edge weights are a bit artificial. Generally, it's not exactly clear what kind of information we get rid off when removing the non-backbone edges -- and in our case it's even less clear because I've made up two of the methods. Thus, we'll still do the exercises during the following weeks using the full network that preserves all of the connection information.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#5 - correlation with length of content\n",
    "\n",
    "import scipy.stats as stats\n",
    "#use the length of content and verify if there is a correlation with the in-degrees and out-degrees\n",
    "word_counts = []\n",
    "for node in rock_artists_graph.nodes():\n",
    "    word_count_value = rock_artists_graph.nodes[node]['word_count']\n",
    "    word_counts.append(word_count_value)\n",
    "\n",
    "correlation_in_degree_length_of_content = stats.pearsonr(list(in_deg.values()), word_counts)\n",
    "correlation_out_degree_length_of_content = stats.pearsonr(list(out_deg.values()), word_counts)\n",
    "\n",
    "correlation_in_degree_length_of_content2 = stats.spearmanr(list(in_deg.values()), word_counts)\n",
    "correlation_out_degree_length_of_content2 = stats.spearmanr(list(out_deg.values()), word_counts)\n",
    "\n",
    "print(f\"Using the Pearson correlation:\\n\")\n",
    "print(f\"The correlation between in-degrees and length of content is: \", correlation_in_degree_length_of_content.statistic)\n",
    "print(f\"The correlation between out-degrees and length of content is: \", correlation_out_degree_length_of_content.statistic)\n",
    "\n",
    "print(f\"\\nUsing the Spearman correlation:\\n\")\n",
    "print(f\"The correlation between in-degrees and length of content is: \", correlation_in_degree_length_of_content2.statistic)\n",
    "print(f\"The correlation between out-degrees and length of content is: \", correlation_out_degree_length_of_content2.statistic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deg_centrality = nx.degree_centrality(rock_artists_graph)\n",
    "top5_deg_centrality = sorted(deg_centrality.items(), key=lambda x: x[1], reverse=True)[:5]\n",
    "\n",
    "print(f\"Top 5 most central asrtist based on degree centrality: \")\n",
    "for node, centrality in top5_deg_centrality:\n",
    "    print(f\"{node}: {centrality}\")\n",
    "\n",
    "betweenness_centrality = nx.betweenness_centrality(rock_artists_graph)\n",
    "top5_betweenness_centrality =  sorted(betweenness_centrality.items(), key=lambda x: x[1], reverse=True)[:5]\n",
    "print(f\"\\nTop 5 most central artist based on betweenness centrality: \")\n",
    "for node, centrality in top5_betweenness_centrality:\n",
    "    print(f\"{node}: {centrality}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We create a weighted undirected graph based on the betweenness centrality\n",
    "weighted_rock_artist_graph1 = rock_artists_graph.to_undirected(reciprocal=False, as_view=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weighted network 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We calculate the betweenness centrality for each node. It shows how often a node appears on the shortest paths between other nodes in the network.\n",
    "betweenness_centrality_w1 = nx.betweenness_centrality(weighted_rock_artist_graph1)\n",
    "\n",
    "#we find the maximum and minimum betweenness centrality values, so later we can normalize the values between 1 and 100\n",
    "max_bc1 = max(betweenness_centrality_w1.values())\n",
    "min_bc1 = min(betweenness_centrality_w1.values())\n",
    "\n",
    "#We loop through each egdge from u to v. u is the source node and v is the target node\n",
    "for u, v in weighted_rock_artist_graph1.edges():\n",
    "    #the source_bc variable shows how important the connetion is\n",
    "    source_bc1 = betweenness_centrality_w1[u]\n",
    "\n",
    "    if max_bc1 == min_bc1:\n",
    "        weight = 1.0\n",
    "    else:\n",
    "        #We normalize the weight between 1 and 100\n",
    "        weight = 1 + (source_bc1 - min_bc1) / (max_bc1 - min_bc1)*99\n",
    "    #We assign the weight to the edge\n",
    "    weighted_rock_artist_graph1[u][v]['weight'] = weight\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#assortavity coefficient of weighted network 1\n",
    "\n",
    "assortavity_coefficient_w1 = nx.degree_assortativity_coefficient(weighted_rock_artist_graph1)\n",
    "print(\"The assortavity coefficient of the weighted rock artists graph based on betweenness centrality is: \", assortavity_coefficient_w1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The assortavity coefficient shows, that the nodes connect to other nodes with different properies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting the network using forceatlas2 layout, the size according to the total degree.\n",
    "\n",
    "node_size_total_degree = [weighted_rock_artist_graph1.degree(node) * 10 for node in weighted_rock_artist_graph1.nodes()]\n",
    "\n",
    "pos = nx.forceatlas2_layout(weighted_rock_artist_graph1, gravity = 10)\n",
    "plt.figure(1, figsize=(15,15))\n",
    "nx.draw_networkx(weighted_rock_artist_graph1, pos, width=0.1, node_size = node_size_total_degree, font_size= 8)\n",
    "plt.show()\n",
    "display(Markdown(\"**Figure 2:** *Plot of the weighted rock artist graph based on the betweenness centrality*\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weighted network 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weighted_rock_artist_graph2 = rock_artists_graph.to_undirected(reciprocal=False, as_view=True)\n",
    "\n",
    "for u, v in weighted_rock_artist_graph2.edges():\n",
    "    weighted_rock_artist_graph2[u][v]['weight'] = 1/ weighted_rock_artist_graph1[u][v]['weight']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We calculate the betweenness centrality on the weighted graph 2\n",
    "betweenness_centrality_w2 = nx.betweenness_centrality(weighted_rock_artist_graph2)\n",
    "\n",
    "max_bc2 = max(betweenness_centrality_w2.values())\n",
    "min_bc2 = min(betweenness_centrality_w2.values()) \n",
    "\n",
    "for i, v in weighted_rock_artist_graph2.edges():\n",
    "    source_bc2 = betweenness_centrality_w2[i]\n",
    "\n",
    "    if max_bc2 == min_bc2:\n",
    "        weight = 1.0\n",
    "    else:\n",
    "        weight = 1 + (source_bc2 - min_bc2) / (max_bc2 - min_bc2)*99\n",
    "    weighted_rock_artist_graph2[i][v]['weight'] = weight\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we create a table. 3 column: src, trg, weight\n",
    "#we use tab as separator\n",
    "#we save the table to a csv file\n",
    "import pandas as pd\n",
    "edge_data = []\n",
    "with open(\"../files/weighted_network1.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"src\\ttrg\\tweight\\n\")\n",
    "    for u, v , data in weighted_rock_artist_graph1.edges(data=True):\n",
    "        weight = data.get('weight', 1.0)\n",
    "        f.write(f\"{u}\\t{v}\\t{data['weight']}\\n\")\n",
    "        edge_data.append({\"src\": u, \"trg\": v, \"weight\": weight})\n",
    "\n",
    "df = pd.DataFrame(edge_data)\n",
    "df.to_csv(\"../files/weighted_rock_artists_graph_bc.csv\", sep=\"\\t\", index=False)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import backboning as bn\n",
    "\n",
    "df_edges, _, _ = bn.read(\"../files/weighted_rock_artists_graph_bc.csv\", \"weight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "backbone_w1 = bn.disparity_filter(df_edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "backbone_w1[backbone_w1['score'] > 0].hist(bins=30)  # You can adjust the number of bins\n",
    "plt.xlabel('Score')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Histogram of Scores')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from backboning import disparity_filter\n",
    "import pandas as pd\n",
    "\n",
    "backbone_df = disparity_filter(df_edges)  \n",
    "#backbone_w1 = nx.from_pandas_edgelist(backbone_df, source=\"src\", target=\"trg\", edge_attr=\"weight\")\n",
    "\n",
    "print(\"Number of edges in the original graph:\", weighted_rock_artist_graph1.number_of_edges())\n",
    "print(\"Number of edges in the backbone:\", backbone_w1.number_of_edges())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import networkx as nx\n",
    "from backboning import disparity_filter\n",
    "\n",
    "# disparity_filter expects column name 'nij'\n",
    "df_edges = df_edges.rename(columns={'weight': 'nij'})\n",
    "\n",
    "backbone_df = disparity_filter(df_edges)                  # returns columns ['src','trg','nij','score',...]\n",
    "# optional: threshold by score (choose alpha you like)\n",
    "backbone_df = backbone_df[backbone_df['score'] > 0.62]\n",
    "\n",
    "# create network using the backbone 'score' (or 'nij' if you prefer)\n",
    "backbone_w1 = nx.from_pandas_edgelist(backbone_df, source=\"src\", target=\"trg\", edge_attr=\"score\")\n",
    "\n",
    "print(\"Number of edges in the original graph:\", weighted_rock_artist_graph1.number_of_edges())\n",
    "print(\"Number of edges in the backbone:\", backbone_w1.number_of_edges())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we visualize the network\n",
    "#plotting the network using forceatlas2 layout, the size according to the total degree.\n",
    "\n",
    "# node_size_total_degree = [weighted_rock_artist_graph1.degree(node) * 10 for node in weighted_rock_artist_graph1.nodes()]\n",
    "\n",
    "# pos = nx.forceatlas2_layout(weighted_rock_artist_graph1, gravity = 10)\n",
    "# plt.figure(1, figsize=(15,15))\n",
    "# nx.draw_networkx(weighted_rock_artist_graph1, pos, width=0.1, node_size = node_size_total_degree, font_size= 8)\n",
    "# plt.show()\n",
    "# display(Markdown(\"**Figure 2:** *Plot of the weighted rock artist graph based on the betweenness centrality*\"))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
