{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Formalia:\n",
    "\n",
    "Please read the [assignment overview page](https://github.com/suneman/socialgraphs2025/wiki/Assignments) carefully before proceeding. This page contains information about formatting (including formats etc), group sizes, and many other aspects of handing in the assignment. \n",
    "\n",
    "_If you fail to follow these simple instructions, it will negatively impact your grade!_\n",
    "\n",
    "**Due date and time**: The assignment is due on Tuesday September 30th, 2025 at 23:55. Hand in your IPython notebook file (with extension `.ipynb`) via DTU Learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1.1: Exploring WS and BA models\n",
    "\n",
    "This first part draws on the Watts-Stogatz and Barabasi-Albert models from Week 3. You should provide solutions to the exercises with the following titles from **Part 1** \n",
    "\n",
    "* *Did you really read the text? Answer the following questions (no calculations needed) in your IPython notebook*\n",
    "\n",
    "* *WS edition*\n",
    "\n",
    "And from **Part 2**\n",
    "\n",
    "* *BA Edition*.\n",
    "  * **Note**: The second part of this exercise (after the questions to the text) first has you build a BA network step-by-step, but doesn't ask any questions. For that part, I would simply like you to write well-documented code that shows how you build the network. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise Part 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> *Exercises*: Did you really read the text? Answer the following questions (no calculations needed) in your IPython notebook.\n",
    ">\n",
    "> * What's the problem with random networks as a model for real-world networks according to the argument in section 3.5 (near the end)?\n",
    "> * List the four regimes that characterize random networks as a function of $\\langle k \\rangle$.\n",
    "> * According to the book, why is it a problem for random networks (in terms of being a model for real-world networks) that the degree-dependent clustering $C(k)$ decreases as a function of $k$ in real-world networks?"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import networkx as nx\n",
    "from networkx.generators.directed import scale_free_graph\n",
    "\n",
    "## Generating 3 graphs with N=500, <k>=4 and p= 0, 0.1 and 1, using`nx.watts_strogatz_graph`. Calculating the average shortest path length <d> for each one.\n",
    "\n",
    "probabilities = [0, 0.1, 1]\n",
    "networks = []\n",
    "\n",
    "print(\"Average shortest path lengths for the different probabilities:\")\n",
    "for i in range (len(probabilities)):\n",
    "    g = nx.watts_strogatz_graph(500, 4, probabilities[i])\n",
    "    networks.append(g)\n",
    "    print(\"For probability \", probabilities[i] , \" the average shortest path length is: \", nx.average_shortest_path_length(g))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Describe what happens to the network when p = 1:\n",
    "As p in Watto-Strogatz Model is the probability for each link to rewire to a randomly chosen node, so if p=1 it means that the network becomes a random network. (As the links are random.)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "##Generating a lot of networks with different values of p.\n",
    "\n",
    "random_probabilities = [random.random() for _ in range(10)]\n",
    "random_probabilities.sort()\n",
    "average_shortest_path_lengths = []\n",
    "for p in random_probabilities:\n",
    "    g = nx.watts_strogatz_graph(500, 4, p)\n",
    "    average_shortest_path_lengths.append(nx.average_shortest_path_length(g))\n",
    "\n",
    "plt.plot(random_probabilities, average_shortest_path_lengths, marker='x')   # line plot with points\n",
    "plt.xlabel(\"probabilities\")\n",
    "plt.ylabel(\"Average shortest path lengths\")\n",
    "plt.title(\"Plot of two arrays\")\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0.001‹p‹0.01 is the value of p for which the average shortest path length gets close to the short paths we find in a fully randomized network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also see that the average shortest path is shorter and shorter with probabilities which are closer to 1.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "\n",
    "N = 500\n",
    "k = 4\n",
    "p =[0, 0.01, 0.03, 0.05, 0.1, 0.2]\n",
    "dictionary = {}\n",
    "standard_deviations = {}\n",
    "\n",
    "for i in p:\n",
    "    average_shortest_path_lengths_with_d = []\n",
    "    for j in range(50):\n",
    "        g = nx.watts_strogatz_graph(N, k, i)\n",
    "        average_shortest_path_lengths_with_d.append(nx.average_shortest_path_length(g))\n",
    "    standard_deviations[i] = np.std(average_shortest_path_lengths_with_d)\n",
    "    dictionary[i] = np.mean(average_shortest_path_lengths_with_d)\n",
    "\n",
    "print(standard_deviations)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "keys = sorted(dictionary.keys())\n",
    "values = [dictionary[k] for k in keys]\n",
    "y_error = [standard_deviations[k] for k in keys]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "\n",
    "# Plot with error bars\n",
    "plt.errorbar(keys, values, yerr=y_error, fmt='-o', capsize=5, label='Standard deviation as error bars')\n",
    "plt.xlabel(\"Rewiring Probability\")\n",
    "plt.ylabel(\"Average shortest path length\")\n",
    "plt.title(\"Average shortest path length vs Rewiring Probability\")\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On this plot we can see the relationship between the rewiring probability. On the y-axis you can see the average shortest path lengths of random generated graphs with the rewiring probability from the x-axis. For the error bars we used the standard deviation of the average shortest path lengths. As we can see the average shortest path length decreases very quickly even with a little bit of rewiring."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# We made a graph and added two nodes and an edge between them. The name of the nodes will be integers from 0 to n-1.\n",
    "BA_Graph = nx.Graph()\n",
    "BA_Graph.add_edge(0, 1)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# We then added 98 more nodes. Each new node has one edge that connects to the previous nodes. The node to which the new node connects is chosen randomly by their proportion to their degree. This means that in a way that if a node has more connection, there is a bigger change that the new node will connect to that node.\n",
    "\n",
    "import random\n",
    "for i in range (2, 100):\n",
    "    edges = list(BA_Graph.edges()) # list of all edges in the current graph\n",
    "    flat_edges = [node for edge in edges for node in edge] #I flatten the list of edges, so that each node appears as many times as its degree\n",
    "    new_node = i # name of the new node\n",
    "    connected_node = random.choice(flat_edges) # we chosed a random node from the flat_edges list\n",
    "    BA_Graph.add_edge(new_node, connected_node)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# visualizing the network\n",
    "nx.draw_networkx(BA_Graph)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# I add more nodes, so that in total there are 5000 nodes in the network in the same way as above\n",
    "for i in range (101, 5000):\n",
    "    edges = list(BA_Graph.edges())\n",
    "    flat_edges = [node for edge in edges for node in edge]\n",
    "    new_node = i\n",
    "    connected_node = random.choice(flat_edges)\n",
    "    BA_Graph.add_edge(new_node, connected_node)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "degrees = dict(BA_Graph.degree())\n",
    "min_degree = min(degrees.values())\n",
    "max_degree = max(degrees.values())\n",
    "\n",
    "print(\"The minimum degree is: \", min_degree)\n",
    "print(\"The maximum degree is: \", max_degree)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# We make a histogram of the degrees_array (degree distribution)\n",
    "import numpy as np\n",
    "degrees_array = np.array(list(degrees.values()))\n",
    "degrees_histogram_linear, degrees_bin_linear = np.histogram(degrees_array, bins=(max_degree - min_degree + 1)) #for bins we used the range of degrees between minimum and maximum degree"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# We plot the degree distribution on a linear scale\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(degrees_histogram_linear, linestyle = 'dotted')\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# We plot the degree distribution on a log-log scale\n",
    "degrees_histogram_log, bin_log = np.histogram(degrees_array, bins='auto') # 'auto' lets numpy choose the bin size automatically\n",
    "bin_centers = 0.5 * (bin_log[1:] + bin_log[:-1])\n",
    "plt.figure(figsize=(7,5))\n",
    "plt.scatter(bin_centers, degrees_histogram_log, s=20, c=\"blue\", marker=\"o\")\n",
    "plt.xscale(\"log\")\n",
    "plt.yscale(\"log\")\n",
    "plt.xlabel(\"k\")\n",
    "plt.ylabel(\"count\")\n",
    "plt.title(\"Histogram (log-log)\")\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1.2: Stats and visualization of the Rock Music Network\n",
    "\n",
    "This second part requires you to have built the network of Rock Musicians as described in the exercises for Week 4. You should complete the following exercise from **Part 2**.\n",
    "\n",
    "* *Explain your process in words*\n",
    "\n",
    "* *Simple network statistics and analysis*.\n",
    "\n",
    "  * **Note related to this and the following exercise**. It is nice to have the dataset underlying the statistics and visualization available when we grade. Therefore, I recommend that you create a small *network dataset*, which is simply your graph stored in some format that you like (since it's only a few hundred nodes and a few thousand edges, it won't take up a lot of space). You can then place that network one of your group members' GitHub account (or some other server that's available online) and have your Jupyter Notebook fetch that dataset when it runs. (It's OK to use an LLM for help with setting this up, if it seems difficult). \n",
    "\n",
    "And the following exercise from **Part 3**\n",
    "\n",
    "* *Let's build a simple visualization of the network*\n",
    "\n",
    "And that's it! You're all set."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import requests\n",
    "import os\n",
    "from urllib.parse import quote\n",
    "import time\n",
    "import json\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import re\n",
    "#We made a wikipediadownloader class that downloads and saves the json version of a wikipedia page\n",
    "\n",
    "class WikipediaDownloader:\n",
    "    def __init__(self):\n",
    "        self.base_url = \"https://en.wikipedia.org/w/api.php\"\n",
    "\n",
    "        self.session = requests.Session()\n",
    "        self.session.headers.update({\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "        })\n",
    "\n",
    "    def download_page_html(self, title):\n",
    "        try:\n",
    "            params = {\n",
    "                \"action\": \"query\",\n",
    "                \"prop\": \"revisions\",\n",
    "                \"rvprop\": \"content\",\n",
    "                \"format\": \"json\",\n",
    "                \"titles\": title.replace(' ', '_'),  # Use the actual title parameter\n",
    "                \"rvslots\": \"main\"\n",
    "            }\n",
    "            print(f\"Downloading: {title}\")\n",
    "            response = self.session.get(self.base_url, params=params)\n",
    "\n",
    "            response.raise_for_status()\n",
    "            return response.text\n",
    "        except requests.RequestException as e:\n",
    "            print(f\"Error downloading {title}: {e}\")\n",
    "            return None\n",
    "\n",
    "    def save_page(self, title, content):\n",
    "        if content is None:\n",
    "            return False\n",
    "\n",
    "        # Create directory if it doesn't exist\n",
    "        os.makedirs(\"wikipedia_pages\", exist_ok=True)\n",
    "\n",
    "        # Clean filename\n",
    "        safe_filename = \"\".join(c for c in title if c.isalnum() or c in (' ', '-', '_')).rstrip()\n",
    "        filename = f\"wikipedia_pages/{safe_filename}.json\"\n",
    "\n",
    "        try:\n",
    "            with open(filename, 'w', encoding='utf-8') as f:\n",
    "                f.write(content)\n",
    "            print(f\"Saved: {filename}\")\n",
    "            return filename\n",
    "        except IOError as e:\n",
    "            print(f\"Error saving {title}: {e}\")\n",
    "            return False\n",
    "\n",
    "    def download_and_save(self, title):\n",
    "        content = self.download_page_html(title)\n",
    "\n",
    "        if content:\n",
    "            return self.save_page(title, content)\n",
    "        return False\n",
    "\n",
    "downloader = WikipediaDownloader()\n",
    "\n",
    "favorite_pages = [\n",
    "    \"List of mainstream rock performers\"\n",
    "]\n",
    "for page_title in favorite_pages:\n",
    "    print(f\"\\n--- Processing: {page_title} ---\")\n",
    "\n",
    "    # Download HTML version\n",
    "    file_name = downloader.download_and_save(page_title)\n",
    "    if(file_name):\n",
    "        print(f\"Successfully downloaded and saved: {file_name}\")\n",
    "    else:\n",
    "        print(f\"Failed to download or save: {page_title}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "#We extracted the artists from the mainstreem rock performers wiki\n",
    "import json\n",
    "with open(\"wikipedia_pages/List of mainstream rock performers.json\", \"r\") as f:\n",
    "    html_content = f.read()\n",
    "\n",
    "#we had to decode the json because of the special characters\n",
    "parsed_json = json.loads(html_content)\n",
    "page_id = list(parsed_json['query']['pages'].keys())[0]\n",
    "wiki_text = parsed_json['query']['pages'][page_id]['revisions'][0]['slots']['main']['*']\n",
    "\n",
    "# Now extract from the properly decoded text\n",
    "pattern = r'\\[\\[([^\\]|]+)'"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "\n",
    "import re\n",
    "# Find the artists and saved them to the artists.txt\n",
    "matches = re.findall(pattern, wiki_text)\n",
    "\n",
    "# Optional: remove duplicates and sort\n",
    "artists = sorted(set(matches))\n",
    "\n",
    "# Save to a text file\n",
    "with open(\"artists.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for artist in artists:\n",
    "        f.write(artist + \"\\n\")\n",
    "\n",
    "print(f\"Extracted {len(artists)} artists and saved to artists.txt\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "#We had to remove the last line because it was not an artist\n",
    "with open(\"artists.txt\", \"r\") as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "with open(\"artists.txt\", \"w\") as file:\n",
    "    for line in lines[:-1]:\n",
    "        file.write(line)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Open the file in read mode\n",
    "artists_file_names = []\n",
    "with open(\"artists.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    for line in file:\n",
    "        artist = line.strip()  # remove leading/trailing spaces and newline\n",
    "        file_name = downloader.download_and_save(artist)\n",
    "        if(file_name):\n",
    "            artists_file_names.append(file_name)\n",
    "        else:\n",
    "            print(f\"Failed to download or save: {page_title}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "#we saved the artists to an artist_array so that it will be easier to iterate through them later\n",
    "artist_array = []\n",
    "with open(\"artists.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    for line in file:\n",
    "        artist_array.append(line.strip())  # remove leading/trailing spaces and newline"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# We made a function that checks if an artist is referenced in another artist's wikipedia page\n",
    "def find_referenced_artists(artist_to_find, current_artist, current_artist_file_name):\n",
    "    file_path = current_artist_file_name\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        wiki_html = f.read()\n",
    "\n",
    "    # Find all wiki links with regex\n",
    "    pattern = r\"\\[\\[\" + re.escape(artist_to_find) + r\"(?:\\|[^\\]]+)?\\]\\]\"\n",
    "\n",
    "    if re.search(pattern, wiki_html):\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "#We made a directed graph and added the artists as nodes and if an artist referenced another artist in his wikipedia page we added a directed edge from the first artist to the second artist.\n",
    "rock_artists_graph = nx.DiGraph()\n",
    "\n",
    "for artist in artist_array:\n",
    "    rock_artists_graph.add_node(artist)\n",
    "\n",
    "for index in range (len(artist_array)):\n",
    "    artist = artist_array[index]\n",
    "    artist_file_name = artists_file_names[index]\n",
    "    for artist_to_find in artist_array:\n",
    "        if artist != artist_to_find:\n",
    "            if find_referenced_artists(artist_to_find, artist, artist_file_name):\n",
    "                rock_artists_graph.add_edge(artist, artist_to_find)\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "#after analysing the data, we found a couple of strings that needed to be removed\n",
    "rock_artists_graph.remove_node('Category:Lists of rock musicians')\n",
    "rock_artists_graph.remove_node('Category:Lists of rock musicians by subgenre')\n",
    "rock_artists_graph.remove_node('AllMusic')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "#We made a helper function that extracts the text from the Wikipedia json structure\n",
    "#We used llm to help us with this 2 functions\n",
    "def extract_wiki_text(data):\n",
    "    pages = data[\"query\"][\"pages\"]\n",
    "    first_page = next(iter(pages.values()))\n",
    "    return first_page[\"revisions\"][0][\"slots\"][\"main\"][\"*\"]\n",
    "\n",
    "#We made a function that returns the number of words in a json file based on the regex as below\n",
    "def word_count(current_artist_file_name):\n",
    "    file_path = Path(current_artist_file_name)\n",
    "    with file_path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    text = extract_wiki_text(data)\n",
    "    \n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "\n",
    "    words = re.findall(r\"\\b[\\w’-]+\\b\", text, flags=re.UNICODE)\n",
    "    \n",
    "    return len(words)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "#We connect the artists_file_names with the artist_arrays to create a dictionary\n",
    "artists_to_path = dict(zip((artist_array), (artists_file_names)))\n",
    "\n",
    "word_count_array = []\n",
    "for artist in rock_artists_graph.nodes():\n",
    "    file_path = artists_to_path[artist]\n",
    "    word_num = word_count(file_path)\n",
    "    word_count_array.append(word_num)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "artists_to_dict = {}\n",
    "for index, item in enumerate(list(rock_artists_graph.nodes.items())):\n",
    "    artists_to_dict[item[0]] = word_count_array[index]\n",
    "\n",
    "nx.set_node_attributes(rock_artists_graph, values = artists_to_dict, name = \"word_count\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "#We made a function that disconnect nodes that don't have any edges\n",
    "def disconnect_node(graph_name):\n",
    "    nodes_with_no_out_degree = []\n",
    "    for node in graph_name:\n",
    "        if graph_name.out_degree(node) == 0 and graph_name.in_degree(node) == 0:\n",
    "            nodes_with_no_out_degree.append(node)\n",
    "\n",
    "    graph_name.remove_nodes_from(nodes_with_no_out_degree)\n",
    "    disconnected = nodes_with_no_out_degree\n",
    "    return disconnected\n",
    "\n",
    "disconnect = disconnect_node(rock_artists_graph)\n",
    "print(f\"Disconnected nodes: \", disconnect)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#We search for the largest component\n",
    "largest_component_sort = sorted(nx.weakly_connected_components(rock_artists_graph), key=len, reverse=True)\n",
    "largest_component = rock_artists_graph.subgraph(largest_component_sort[0])\n",
    "\n",
    "# add who is the largest component with number of edges and nodes\n",
    "\n",
    "largest_component_nodes = largest_component.number_of_nodes()\n",
    "print(f\"The number of nodes of the largest component is: \", largest_component_nodes)\n",
    "\n",
    "#number of links\n",
    "largest_component_edges = largest_component.number_of_edges()\n",
    "\n",
    "print(\"The number of links in the largest component is:\", largest_component_edges)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#We list the top 5 most connected artists\n",
    "from operator import itemgetter\n",
    "\n",
    "max_in_degree_node = sorted(rock_artists_graph.in_degree, key=itemgetter(1), reverse=True)\n",
    "max_out_degree_node = sorted(rock_artists_graph.out_degree, key=itemgetter(1), reverse=True)\n",
    "\n",
    "top5_in_degree = []\n",
    "top5_out_degree =[]\n",
    "\n",
    "for i in range(5):\n",
    "    top5_in_degree.append(max_in_degree_node[i])\n",
    "\n",
    "for i in range(5):\n",
    "    top5_out_degree.append(max_out_degree_node[i])\n",
    "\n",
    "\n",
    "\n",
    "print(f\"The top 5 bands with the most in_degrees:\")\n",
    "for i in top5_in_degree:\n",
    "    print(i[0])\n",
    "\n",
    "print(f\"\\nThe top 5 bands with the most out_degrees:\")\n",
    "for i in top5_out_degree:\n",
    "    print(i[0])\n",
    "\n",
    "print(f\"\\n\\nI assumed that the result would contain popular bands, but I expected more overlap\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#We list 10 pages with the longest entry\n",
    "wiki_page_length = []\n",
    "\n",
    "for node in rock_artists_graph.nodes():\n",
    "    word_c = rock_artists_graph.nodes[node][\"word_count\"]\n",
    "    wiki_page_length.append((node, word_c))\n",
    "\n",
    "wiki_page_length = sorted(wiki_page_length, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "top10_wiki_page_length = wiki_page_length[:10]\n",
    "\n",
    "print(\"Top 10 longest wiki pages:\")\n",
    "for node, word_c in top10_wiki_page_length:\n",
    "    print(node, word_c)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": ""
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(\"The number of nodes in the network is: \", rock_artists_graph.number_of_nodes())\n",
    "print(\"The number of edges in the network is: \", rock_artists_graph.number_of_edges())"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "\n",
    "rock_artists_graph_attributes ={\"graph\":rock_artists_graph, \"in_degrees\":[], \"out_degrees\":[], \"min_degree\":0, \"max_degree\":0}\n",
    "\n",
    "scale_free_graph_attributes = {\"graph\":nx.scale_free_graph(rock_artists_graph.number_of_nodes()), \"in_degrees\":[], \"out_degrees\":[], \"min_degree\":0, \"max_degree\":0}\n",
    "\n",
    "random_graph_attributes = {\"graph\":nx.gnm_random_graph(rock_artists_graph.number_of_nodes(), rock_artists_graph.number_of_edges(), directed=True), \"in_degrees\":[], \"out_degrees\":[], \"min_degree\":0, \"max_degree\":0}\n",
    "\n",
    "\n",
    "graphs = [rock_artists_graph_attributes, scale_free_graph_attributes, random_graph_attributes]\n",
    "\n",
    "for graph in graphs:\n",
    "    in_degrees_array = []\n",
    "    out_degrees_array = []\n",
    "    for node in graph[\"graph\"].nodes():\n",
    "        in_degree = graph[\"graph\"].in_degree(node)\n",
    "        out_degree = graph[\"graph\"].out_degree(node)\n",
    "        in_degrees_array.append(in_degree)\n",
    "        out_degrees_array.append(out_degree)\n",
    "    graph[\"in_degrees\"] = in_degrees_array\n",
    "    graph[\"out_degrees\"] = out_degrees_array\n",
    "    degrees = dict(graph[\"graph\"].degree())\n",
    "    graph[\"min_degree\"] = min(degrees.values())\n",
    "    graph[\"max_degree\"] = max(degrees.values())\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "sns.histplot(rock_artists_graph_attributes[\"in_degrees\"], bins=(rock_artists_graph_attributes[\"max_degree\"] - rock_artists_graph_attributes[\"min_degree\"] + 1), kde=True)\n",
    "plt.title(\"Distribution of In-Degrees of the Rock Artists Graph\")\n",
    "plt.show()\n",
    "\n",
    "# Plot histogram + density curve\n",
    "sns.histplot(rock_artists_graph_attributes[\"out_degrees\"], bins=(rock_artists_graph_attributes[\"max_degree\"] - rock_artists_graph_attributes[\"min_degree\"] + 1), kde=True)\n",
    "plt.title(\"Distribution of Out-Degrees of the Rock Artists Graph\")\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The in-degree distribution has a long tail, meaning that there are a few nodes with very high in-degrees, which probably means that a lot of artists' wikipedia pages referred to the same artist. In the out-degree distributon most nodes has an out-degree between 5 and 10, which means they refer to similar number of artists. This is because most artists' wikipedia pages referred to the same number of other artists, but only a few artists get referred to by a lot of other artists."
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "# Plot histogram + density curve\n",
    "sns.histplot(rock_artists_graph_attributes[\"in_degrees\"], bins=(rock_artists_graph_attributes[\"max_degree\"] - rock_artists_graph_attributes[\"min_degree\"] + 1), kde=True)\n",
    "plt.title(\"Distribution of In-Degrees of the Rock Artists Graph\")\n",
    "plt.show()\n",
    "\n",
    "sns.histplot(scale_free_graph_attributes[\"in_degrees\"], bins=(scale_free_graph_attributes[\"max_degree\"] - scale_free_graph_attributes[\"min_degree\"] + 1), kde=True)\n",
    "plt.title(\"Distribution of In-Degrees of the Scale-free Graph\")\n",
    "plt.show()\n",
    "\n",
    "# We plot the degree distribution on a log-log scale\n",
    "scale_free_graph_degrees_histogram_log, scale_free_graph_bin_log = np.histogram(scale_free_graph_attributes[\"in_degrees\"], bins='auto') # 'auto' lets numpy choose the bin size automatically\n",
    "\n",
    "print(\"As there are a lot of degrees which are 0, and a few which are relatively big, it is better to plot the histogram on a log-log scale.\")\n",
    "\n",
    "scale_free_graph_bin_centers = 0.5 * (scale_free_graph_bin_log[1:] + scale_free_graph_bin_log[:-1])\n",
    "plt.figure(figsize=(7,5))\n",
    "plt.scatter(scale_free_graph_bin_centers, scale_free_graph_degrees_histogram_log, s=20, c=\"blue\", marker=\"o\")\n",
    "plt.xscale(\"log\")\n",
    "plt.yscale(\"log\")\n",
    "plt.xlabel(\"degree\")\n",
    "plt.ylabel(\"count\")\n",
    "plt.title(\"A Histogram of the In-Degrees of a Scale-Free Graph (log-log)\")\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The in-degree distribution of the rock artists graph is similar to the in-degree distribution of a scale-free graph, as both have a long tail. This means that there are a few nodes with very high in-degrees in both graphs. However, the rock artist graph does not grow that gradually such as the scale-free graph."
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sns.histplot(rock_artists_graph_attributes[\"out_degrees\"], bins=(rock_artists_graph_attributes[\"max_degree\"] - rock_artists_graph_attributes[\"min_degree\"] + 1), kde=True)\n",
    "plt.title(\"Distribution of Out-Degrees of the Rock Artists Graph\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "sns.histplot(random_graph_attributes[\"out_degrees\"], bins=(random_graph_attributes[\"max_degree\"] - random_graph_attributes[\"min_degree\"] + 1), kde=True)\n",
    "plt.title(\"Distribution of Out-Degrees of a Random Graph\")\n",
    "plt.show()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The out-degree distribution of the rock artists graph is similar to the out-degree distribution of a random graph in case of appearance.  However, the ranges differ as the rock graph out-degrees range from 5 to 70, while the random graph out-degrees range from 5 to 30."
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The hardest part of the exercise was to compose the regexes for the wikipedia json files. Moreover, merging the jupyter notebook was hard. Next time before pushing them to Github we will clear all the outputs."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3 - Visualizing the networks"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import colormaps\n",
    "import matplotlib as mpl\n",
    "import numpy as np\n",
    "\n",
    "graph_color_map = colormaps[\"plasma\"]\n",
    "\n",
    "colors_rgba = graph_color_map(np.linspace(0, 1, 10))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#We create a figure to show which colormap is used for the visualisation\n",
    "figure, axis = plt.subplots(figsize=(7,1))\n",
    "axis.set_title(\"Colormap for the graph representation\")\n",
    "\n",
    "\n",
    "in_wiki_pages = []\n",
    "for i in wiki_page_length:\n",
    "    in_wiki_pages.append(i[1])\n",
    "\n",
    "max_num_for_gradient = max(in_wiki_pages)\n",
    "print(max_num_for_gradient)\n",
    "\n",
    "gradient = np.linspace(0,1,max_num_for_gradient)\n",
    "gradient = np.vstack((gradient, gradient))\n",
    "axis.imshow(gradient, aspect='auto', cmap=graph_color_map)\n",
    "\n",
    "node_colors = []\n",
    "for node in rock_artists_graph.nodes():\n",
    "    grad_word_count = rock_artists_graph.nodes[node]['word_count']\n",
    "    node_colors.append(grad_word_count)\n",
    "\n",
    "axis.set_yticks([])\n",
    "axis.set_xticks([])\n",
    "\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "#We convert our directed graph to an undirected graph and We keep the original\n",
    "#We suggest that we continue the work with the original network, not with the largest component, as it was mentioned earlier, that we should work with the latter only for further analysis\n",
    "undirected_rock_artist_graph = rock_artists_graph.to_undirected(reciprocal=False, as_view=True)\n",
    "\n",
    "\n",
    "nodesize = rock_artists_graph.degree\n",
    "node_size_array = []\n",
    "for artist in rock_artists_graph.degree():\n",
    "    node_degree = artist[1]\n",
    "    node_size_array.append(node_degree)\n",
    "\n",
    "color_map= []\n",
    "for node in rock_artists_graph:\n",
    "    c = nx.get_node_attributes(rock_artists_graph, word_c, \"word_count\")\n",
    "    color_map.append(c)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "pos = nx.random_layout(rock_artists_graph)\n",
    "plt.figure(1, figsize=(20,20))\n",
    "nx.draw_networkx(rock_artists_graph, pos, width=0.1, node_size = node_size_array, node_color = node_colors, font_size= 4)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Hoang Anh Do (s250220): 1.1 WS edition, 1.1 BA edition, 1.2 Loading wikipedia\n",
    "Bálint Kostyál (s250222): 1.2 Generating the rock network, 1.2 Simple network statistics and analysis\n",
    "Enikő Horninger (s253124): 1.2 Generating the rock network, 1.2 Simple network statistics and analysis, 1.2 Part 3: Visualization\n",
    "\n",
    "We all contributed equally to the assignment.\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
